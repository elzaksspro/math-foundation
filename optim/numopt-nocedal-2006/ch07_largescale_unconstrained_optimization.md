# ch07: Large-Scale Unconstrained Optimization
* consideration
  * storage
  * computational costs
* dealing with the hessian
  * sparse factorization
    * objective functions in large problems often possess a structural property known as partial separability,
      which means they can be decomposed into a sum of simpler functions,
      each of which depends on only a small subspace of R^n
  * approximations to the Newton step using iterative linear algebra techniques
    * Hessian-free manner, without explicit calculation or storage of the Hessian.
    * The Hessian approximations generated by the quasi-Newton approaches of Chapter 6
      are usually dense, even when the true Hessian is sparse, and the cost of storing and working
      with these approximations can be excessive for large n

## 7.1  inexact newton methods
* family of inexact Newton methods
  * obtaining approximations to pk N that are
    inexpensive to calculate but are good search directions or steps
  * based on
    solving (7.1) by using the conjugate gradient (CG) method (see Chapter 5) or the Lanczos method,
    with modifications to handle negative curvature in the Hessian ∇ 2 f k
* The use of iterative methods for (7.1)
  * spares us from concerns about the expense of a direct factorization of the Hessian ∇ 2 fk and
    the fill-in that may occur during this process.
  * we can customize the solution strategy to ensure that the rapid convergence properties
    associated with Newton’s methods are not lost in the inexact version.
  * we can implement these methods in a Hessian-free manner, so that the
    Hessian ∇ 2 fk need not be calculated or stored explicitly at all.

### local convergence of inexact newton methods
TODO

### line search newton-cg method
* aka truncated Newton method
  * consists of repeated application of an iterative optimization algorithm to
    approximately solve Newton's equations, to determine an update to the function's parameters.
  * The inner solver is truncated, i.e., run for only a limited number of iterations.
  * It follows that, for truncated Newton methods to work, the inner solver needs to
    produce a good   approximation in a finite number of iterations;[2]
  * conjugate gradient has been suggested and evaluated as a candidate inner loop.
  * more: https://en.wikipedia.org/wiki/Truncated_Newton_method
* the CG method is designed to solve positive definite systems, and
  the Hessian ∇ 2 f k may have negative eigenvalues when x k is not close to a solution.
  * Therefore, we terminate the CG iteration as soon as
    a direction of negative curvature is generated.
  * This adaptation of the CG method produces a search direction pk that is a descent direction.
  * Moreover, the adaptation guarantees that the fast convergence rate of the pure Newton method is preserved,
    * provided that the step length $\alpha_k = 1$ is used whenever it satisfies the acceptance criteria.
* Algorithm 7.1, a line search algorithm that
  * uses a modification of Algorithm 5.2 (CG)
    as the inner iteration to compute each search direction pk
* When the Hessian is nearly singular,
  * the line search Newton–CG direction can be long and of poor quality,
  * requiring many function evaluations in the line search and
    giving only a small reduction in the function
* Hessian-free Newton methods.
  * does not require explicit knowledge of the Hessian $B_k = \nabla^2 f_k$
  * Rather, it requires only that we can supply Hessian–vector products of
    the form $\nabla^2 f_k d$ for any given vector $d$
* The price we pay for
  bypassing the computation of the Hessian is one new gradient evaluation per CG iteration.

## 7.2 limited-memory quasi-newton methods
* Instead of storing fully dense n × n approximations,
  * they save only a few vectors of length n that represent the
    approximations implicitly.
* main idea of this method is to use curvature
  information from only the most recent iterations to construct the Hessian approximation

### limited-memory bfgs
* Practical experience has shown that modest values of m (between 3 and 20, say)
  often produce satisfactory results.
