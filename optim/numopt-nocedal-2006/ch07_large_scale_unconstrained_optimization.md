# ch07: Large-Scale Unconstrained Optimization
* consideration
  * storage
  * computational costs
* dealing with the hessian
  * sparse factorization
    * objective functions in large problems often possess a structural property known as partial separability,
      which means they can be decomposed into a sum of simpler functions,
      each of which depends on only a small subspace of R^n
  * approximations to the Newton step using iterative linear algebra techniques
    * Hessian-free manner, without explicit calculation or storage of the Hessian.
    * The Hessian approximations generated by the quasi-Newton approaches of Chapter 6
      are usually dense, even when the true Hessian is sparse, and the cost of storing and working
      with these approximations can be excessive for large n

# 7.1 inexact newton methods
* family of inexact Newton methods
  * obtaining approximations to pk N that are
    inexpensive to calculate but are good search directions or steps
  * based on
    solving (7.1) by using the conjugate gradient (CG) method (see Chapter 5) or the Lanczos method,
    with modifications to handle negative curvature in the Hessian ∇ 2 f k
* The use of iterative methods for (7.1)
  * spares us from concerns about the expense of a direct factorization of the Hessian ∇ 2 fk and
    the fill-in that may occur during this process.
  * we can customize the solution strategy to ensure that the rapid convergence properties
    associated with Newton’s methods are not lost in the inexact version.
  * we can implement these methods in a Hessian-free manner, so that the
    Hessian ∇ 2 fk need not be calculated or stored explicitly at all.

## local convergence of inexact newton methods
TODO

## line search newton-cg method
* aka truncated Newton method
  * consists of repeated application of an iterative optimization algorithm to
    approximately solve Newton's equations, to determine an update to the function's parameters.
  * The inner solver is truncated, i.e., run for only a limited number of iterations.
  * It follows that, for truncated Newton methods to work, the inner solver needs to
    produce a good   approximation in a finite number of iterations;[2]
  * conjugate gradient has been suggested and evaluated as a candidate inner loop.
  * more: https://en.wikipedia.org/wiki/Truncated_Newton_method
* the CG method is designed to solve positive definite systems, and
  the Hessian ∇ 2 f k may have negative eigenvalues when x k is not close to a solution.
  * Therefore, we terminate the CG iteration as soon as
    a direction of negative curvature is generated.
  * This adaptation of the CG method produces a search direction pk that is a descent direction.
  * Moreover, the adaptation guarantees that the fast convergence rate of the pure Newton method is preserved,
    * provided that the step length $\alpha_k = 1$ is used whenever it satisfies the acceptance criteria.
* Algorithm 7.1, a line search algorithm that
  * uses a modification of Algorithm 5.2 (CG)
    as the inner iteration to compute each search direction pk
* When the Hessian is nearly singular,
  * the line search Newton–CG direction can be long and of poor quality,
  * requiring many function evaluations in the line search and
    giving only a small reduction in the function
* Hessian-free Newton methods.
  * does not require explicit knowledge of the Hessian $B_k = \nabla^2 f_k$
  * Rather, it requires only that we can supply Hessian–vector products of
    the form $\nabla^2 f_k d$ for any given vector $d$
* The price we pay for
  bypassing the computation of the Hessian is one new gradient evaluation per CG iteration.

# 7.2 limited-memory quasi-newton methods
* Instead of storing fully dense n × n approximations,
  * save only a few vectors of length n that represent the approximations implicitly.
* main idea of this method is
  * to use curvature information from only the **most recent iterations**
    to construct the Hessian approximation

## limited-memory bfgs
* Since the inverse Hessian approximation $H_k$ will generally be dense,
  the cost of storing and manipulating it is prohibitive when the number of variables is large
  * thus: store a modified version of $H_k$ implicitly,
    by storing a certain number (say, m) of the vector pairs $\{s_i, y_i\}$ used in the formulas (7.16)–(7.18)
  * the set of vector pairs includes curvature information from the m most recent iterations
    * modest values of m (between 3 and 20, say) often produce satisfactory results.
* in contrast to the standard BFGS iteration, this initial approximation is
  allowed to vary from iteration to iteration
* Algorithm 7.4 (L-BFGS two-loop recursion).
  * inexpensive computation
  * has the advantage that the multiplication by the
    initial matrix H k 0 is isolated from the rest of the computations, allowing this matrix to be
    chosen freely and to vary between iterations.
* A method for choosing Hk 0 that has proved effective in practice is to set ... Equ. 7.20
* for stable BFGS updating
  * the line search be based on the Wolfe conditions (3.6) or strong Wolfe conditions (3.7)
* Algorithm 7.5 (L-BFGS).
  * tends to be less robust when m is small.
    * the optimal choice of m is problem dependent.
  * is often the approach of choice for large problems in which the true Hessian is not sparse
  * may also outperform Hessian-free Newton methods such as Newton–CG approaches,
    in which Hessian–vector products are calculated by finite differences or automatic differentiation
  * benchmark metric:
    * the number of function and gradient evaluations (nfg) and
    * the total CPU time.
* main weakness of the L-BFGS method
  * it converges slowly on ill-conditioned problems
    * specifically, on problems where the Hessian matrix contains a wide distribution of eigenvalues.
* On certain applications, the nonlinear conjugate gradient methods are
  competitive with limited-memory quasi-Newton methods.

## relationship with conjugate gradient methods
* Limited-memory methods
  * evolved as an attempt to improve nonlinear conjugate gradient methods, and
  * early implementations resembled conjugate gradient methods more than quasi-Newton methods.
* memoryless BFGS iteration:
  * is based on the relationship between the two classes
  * as a variant of Algorithm 7.5 in which $m=1$ and $H_k^0=I$ at each iteration.
* A more direct connection with conjugate gradient methods can be seen if we consider
  the memoryless BFGS formula (7.22) in conjunction with an exact line search,
  * the BFGS formula is related in this way to the Polak–Ribière and Hestenes–Stiefel methods.

## general limited-memory updating
TODO

# 7.5 perspectives and software
* Newton–CG methods have been used successfully to solve large problems in a variety of applications
  * use problem-specific preconditioners.
* A preconditioner for Newton–CG based on limited-memory BFGS approximations
